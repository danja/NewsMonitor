<?xml version="1.0" encoding="UTF-8"?><rdf:RDF
	xmlns="http://purl.org/rss/1.0/"
	xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:admin="http://webns.net/mvcb/"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	>
<channel rdf:about="http://blog.soton.ac.uk/enakting">
	<title>the EnAKTing blog</title>
	<link>http://blog.soton.ac.uk/enakting</link>
	<description>EnAKTing the Unbounded Data Web: Challenges in Web Science</description>
	<dc:date>2012-06-27T15:29:21Z</dc:date>
		<sy:updatePeriod>hourly</sy:updatePeriod>
		<sy:updateFrequency>1</sy:updateFrequency>
	<sy:updateBase>2000-01-01T12:00+00:00</sy:updateBase>
	<admin:generatorAgent rdf:resource="http://wordpress.org/?v=3.7.3" />
	<items>
		<rdf:Seq>
					<rdf:li rdf:resource="http://blog.soton.ac.uk/enakting/2012/06/27/learning-web-development-a-fast-moving-target/"/>
					<rdf:li rdf:resource="http://blog.soton.ac.uk/enakting/2011/05/18/see-uk-released/"/>
					<rdf:li rdf:resource="http://blog.soton.ac.uk/enakting/2011/03/14/geonames/"/>
					<rdf:li rdf:resource="http://blog.soton.ac.uk/enakting/2011/01/28/fast-sparql-xml-results-parser-in-python/"/>
					<rdf:li rdf:resource="http://blog.soton.ac.uk/enakting/2010/10/20/open-data-journalism-and-the-future-of-news/"/>
					<rdf:li rdf:resource="http://blog.soton.ac.uk/enakting/2010/08/23/a-linked-data-web-of-a-million-easy-pieces/"/>
					<rdf:li rdf:resource="http://blog.soton.ac.uk/enakting/2010/08/22/pyclips/"/>
					<rdf:li rdf:resource="http://blog.soton.ac.uk/enakting/2009/11/04/welcome_to_enakting/"/>
				</rdf:Seq>
	</items>
</channel>
<item rdf:about="http://blog.soton.ac.uk/enakting/2012/06/27/learning-web-development-a-fast-moving-target/">
	<title>Learning Web development: A fast-moving target</title>
	<link>http://blog.soton.ac.uk/enakting/2012/06/27/learning-web-development-a-fast-moving-target/</link>
	<dc:date>2012-06-27T15:26:57Z</dc:date>
	<dc:creator><![CDATA[emax]]></dc:creator>
			<dc:subject><![CDATA[code]]></dc:subject>
		<dc:subject><![CDATA[software development]]></dc:subject>
	<description><![CDATA[EnAKTing is enjoying its final Summer of Code and will soon have a full ship of summer interns tasked to forge new tools and tech related to the use, publishing and manipulation of Linked Open Data. With each new set of students, we find ourselves substantially revising our student training curriculum to reflect the new [&#8230;]]]></description>
	<content:encoded><![CDATA[<p>EnAKTing is enjoying its final Summer of Code and will soon have a full ship of summer interns tasked to forge new tools and tech related to the use, publishing and manipulation of <a href="http://linkeddata.org/">Linked Open Data</a>.</p>
<p>With each new set of students, we find ourselves substantially revising our student training curriculum to reflect the new languages, toolkits, platforms, patterns/idioms and methodologies that have become &#8220;best practice&#8221; since the previous summer. It is absolutely astounding how quickly Web development as a practice changes and how quickly developers adopt and share new ideas and experimental tools.</p>
<p>One of our interns, Peter West, an alumnus of last summer&#8217;s enAKTing summer of code and professional independent web developer, has compiled a current guide for those transitioning from non-web languages to Web development. Â He has decided to open it up to all (as a RFC) to engage the Web as a community to help those learning on board. Â  See Peter&#8217;s <a href="https://docs.google.com/document/d/1a7ZrePdLO8HHp3_xDMrZMPlerSSmmdpLXDCuuWpe6z4/edit#heading=h.e9iamerfvaaq" target="_blank">Diving into Web Development (June 2012) here</a>. Â I also keep an updated document of Favourite Toolkits and Libraries for efficient coding, which I callÂ <a href="https://docs.google.com/document/d/1DIHyUturWCSdBBQjwz3oQXmrJxhL_1B7uTgoYoStfkU/edit?authkey=CPHG5eUG&amp;authkey=CPHG5eUG">New Wave Javascript</a>. Â  Both documents are open to public comment; please leave suggestions and ideas and we will add them to the doc!</p>
]]></content:encoded>
	</item>
<item rdf:about="http://blog.soton.ac.uk/enakting/2011/05/18/see-uk-released/">
	<title>See UK Released</title>
	<link>http://blog.soton.ac.uk/enakting/2011/05/18/see-uk-released/</link>
	<dc:date>2011-05-18T22:55:38Z</dc:date>
	<dc:creator><![CDATA[Hugh Glaser]]></dc:creator>
			<dc:subject><![CDATA[linked data]]></dc:subject>
		<dc:subject><![CDATA[Open government data]]></dc:subject>
	<description><![CDATA[See UK (http://apps.seme4.com/see-uk) is a simple visualisation of data that has geographic aspectsÂ and has been published as machine-interpretable Linked Data. This site uses data that has been sourced fromÂ data.gov.uk andÂ processedÂ into Linked Data where necessary, but is also designed to be able to useÂ other sources where available. All the datasets are then enriched, byÂ calculating area totals [&#8230;]]]></description>
	<content:encoded><![CDATA[<p><a href="http://apps.seme4.com/see-uk">See UK</a> (<a href="http://apps.seme4.com/see-uk">http://apps.seme4.com/see-uk</a>) is a simple visualisation of data that has geographic aspectsÂ and has been published as machine-interpretable Linked Data.</p>
<p>This site uses data that has been sourced fromÂ data.gov.uk andÂ processedÂ into Linked Data where necessary, but is also designed to be able to useÂ other sources where available. All the datasets are then enriched, byÂ calculating area totals from point data and inferring aggregate valuesÂ for regions that do not have explicit data values, and furtherÂ enriched by establishing linkage between the datasets. These enriched datasets are available directly from the <a href="http://enakting.org/">EnAKTing Project</a> atÂ <a href="http://crime.rkbexplorer.com/">http://crime.rkbexplorer.com/</a>, <a href="http://transport.psi.enakting.org/">http://transport.psi.enakting.org/</a> and <a href="http://absenteeism.psi.enakting.org/">http://absenteeism.psi.enakting.org/</a>. Some of it can be accessed using the Linked Data API at <a href="http://puelia.psi.enakting.org">http://puelia.psi.enakting.org</a>.</p>
<p>It runs on servers provided by the <a href="http://enakting.org/">EnAKTing Project</a> and <a href="http://www.ecs.soton.ac.uk">Electronics and Computer Science</a> at the <a href="http://www.soton.ac.uk">University of Southampton</a>.</p>
<p>The visualisation provides a view centred on a chosen region of theÂ specified size, and most noticeably gives a &#8220;pie-chart&#8221; that shows theÂ viewer how that region compares with similar regions around it. ItÂ is thus designed to focus on the information most relevant to theÂ user. Colour indicates the &#8220;worst&#8221; (red) and &#8220;best&#8221; (green) areasÂ from those shown. This pie-chart is shown in preference to simplyÂ colouring the map itself, as a coloured map confuses the mapÂ features with the data being visualised.</p>
<p>It also gives some context of the real geographyÂ involved, so that a full picture is seen. The user can navigate byÂ looking and clicking on the pie-chart, or the map, and can thus moveÂ around using whatever view they are taking of the data presentation. AÂ search by postcode functionality is also supported, aiding the user inÂ finding specific locations.</p>
<p>An important aspect of the visualisation is that cross-datasetÂ correlation can be achieved and presented in a natural fashion, asÂ the data can be viewed as normalised by population or area, inÂ addition to the raw values. The user can therefore see how regionsÂ compare in terms of, for example, crime density by population orÂ area, rather than just knowing that their county has little crime,Â and guessing this is because the county has a small population orÂ area.</p>
<p>See UK has been produced as a collaborative activity between Seme4 LtdÂ and members of the EnAKTing project at the University of Southampton.</p>
<p>For further details please contact <a href="mailto:hugh.glaser@seme4.com">Hugh Glaser</a> or <a href="mailto:icm@ecs.soton.ac.uk">Ian Millard</a>;Â feedback on this application is veryÂ welcome.</p>
]]></content:encoded>
	</item>
<item rdf:about="http://blog.soton.ac.uk/enakting/2011/03/14/geonames/">
	<title>Geonames</title>
	<link>http://blog.soton.ac.uk/enakting/2011/03/14/geonames/</link>
	<dc:date>2011-03-14T23:09:49Z</dc:date>
	<dc:creator><![CDATA[Daniel Alexander Smith]]></dc:creator>
			<dc:subject><![CDATA[Uncategorized]]></dc:subject>
	<description><![CDATA[In order to get a list of countries and their alternative names from the Geonames RDF Dump, we can do: SELECT DISTINCT ?country ?name ?altname WHERE {?foo &#60;http://www.geonames.org/ontology#parentCountry&#62; ?country . ?country &#60;http://www.geonames.org/ontology#name&#62; ?name . ?country &#60;http://www.geonames.org/ontology#alternateName&#62; ?altname . FILTER(lang(?altname) = "en") } Which leads to 344 results, which look much like the following (in JSON [&#8230;]]]></description>
	<content:encoded><![CDATA[<p>In order to get a list of countries and their alternative names from the <a href="http://download.geonames.org/all-geonames-rdf.zip">Geonames RDF Dump</a>, we can do:</p>
<pre>
SELECT DISTINCT ?country ?name ?altname
WHERE
{?foo &lt;http://www.geonames.org/ontology#parentCountry&gt; ?country . 
?country &lt;http://www.geonames.org/ontology#name&gt; ?name . 
?country &lt;http://www.geonames.org/ontology#alternateName&gt; ?altname . 
FILTER(lang(?altname) = "en")
}
</pre>
<p>Which leads to 344 results, which look much like the following (in JSON format), as we expect:</p>
<pre>
      {
        "country": { "type": "uri" , "value": "http://sws.geonames.org/2635167/" } ,
        "name": { "type": "literal" , "value": "United Kingdom of Great Britain and Northern Ireland" } ,
        "altname": { "type": "literal" , "xml:lang": "en" , "value": "Britain" }
      } ,
      {
        "country": { "type": "uri" , "value": "http://sws.geonames.org/2635167/" } ,
        "name": { "type": "literal" , "value": "United Kingdom of Great Britain and Northern Ireland" } ,
        "altname": { "type": "literal" , "xml:lang": "en" , "value": "UK" }
      } ,
      {
        "country": { "type": "uri" , "value": "http://sws.geonames.org/2635167/" } ,
        "name": { "type": "literal" , "value": "United Kingdom of Great Britain and Northern Ireland" } ,
        "altname": { "type": "literal" , "xml:lang": "en" , "value": "U.K." }
      } ,
</pre>
]]></content:encoded>
	</item>
<item rdf:about="http://blog.soton.ac.uk/enakting/2011/01/28/fast-sparql-xml-results-parser-in-python/">
	<title>Fast SPARQL XML Results Parser in Python</title>
	<link>http://blog.soton.ac.uk/enakting/2011/01/28/fast-sparql-xml-results-parser-in-python/</link>
	<dc:date>2011-01-28T10:49:29Z</dc:date>
	<dc:creator><![CDATA[Daniel Alexander Smith]]></dc:creator>
			<dc:subject><![CDATA[code]]></dc:subject>
		<dc:subject><![CDATA[software development]]></dc:subject>
		<dc:subject><![CDATA[Uncategorized]]></dc:subject>
		<dc:subject><![CDATA[enakting]]></dc:subject>
		<dc:subject><![CDATA[linkeddata]]></dc:subject>
		<dc:subject><![CDATA[python]]></dc:subject>
		<dc:subject><![CDATA[sparql]]></dc:subject>
		<dc:subject><![CDATA[xml]]></dc:subject>
	<description><![CDATA[For one of our projects we need results from SPARQL endpoints as quickly as possible, with little to no need for validation. As such, I re-wrote our original SPARQL XML results parser to use Expat, the non-validating (and fast) XML parser. The results format is a dict in roughly the same as the bindings part [&#8230;]]]></description>
	<content:encoded><![CDATA[<p>For one of our projects we need results from SPARQL endpoints as quickly as possible, with little to no need for validation.</p>
<p>As such, I re-wrote our original SPARQL XML results parser to use Expat, the non-validating (and fast) XML parser.</p>
<p>The results format is a dict in roughly the same as the bindings part of the SPARQL JSON results format.</p>
<p>Example of use:</p>
<pre>sp = SparqlParser()
results = sp.Parse(xmlstring)</pre>
<p>Code:</p>
<pre>import xml.parsers.expat

# Fast Expat based SPARQL stream parser Copyright (c) 2011 Daniel Alexander Smith, University of Southampton
class SparqlParser:

    def __init__(self):
        self.results = []
        self.current = {}
        self.current_name = ""
        self.current_chars = ""
        self.current_type = ""
        self.getting_chars = False
        self.parser = xml.parsers.expat.ParserCreate()
        self.parser.StartElementHandler = self.start_element
        self.parser.EndElementHandler = self.end_element
        self.parser.CharacterDataHandler = self.char_data

    def start_element(self, name, attrs):
        if name == 'binding':
            self.current_name = attrs['name']
        if name == 'literal':
            self.current_type = 'literal'
            self.getting_chars = True
        if name == 'bnode':
            self.current_type = 'bnode'
            self.getting_chars = True
        if name == 'uri':
            self.current_type = 'uri'
            self.getting_chars = True

    def end_element(self, name):
        if name == 'binding':
            self.current[self.current_name] = {'value': self.current_chars, 'type': self.current_type}
            self.current_chars = ""
        if name == 'literal':
            self.getting_chars = False
        if name == 'bnode':
            self.getting_chars = False
        if name == 'uri':
            self.getting_chars = False
        if name == 'result':
            self.results.append(self.current)
            self.current = {}

    def char_data(self, data):
        if self.getting_chars:
            self.current_chars = self.current_chars + data

    def Parse(self, data):
        self.parser.Parse(data, 0)
        return self.results</pre>
]]></content:encoded>
	</item>
<item rdf:about="http://blog.soton.ac.uk/enakting/2010/10/20/open-data-journalism-and-the-future-of-news/">
	<title>Data Journalism and its role for open government data</title>
	<link>http://blog.soton.ac.uk/enakting/2010/10/20/open-data-journalism-and-the-future-of-news/</link>
	<dc:date>2010-10-20T14:53:35Z</dc:date>
	<dc:creator><![CDATA[emax]]></dc:creator>
			<dc:subject><![CDATA[Citizen-sourcing]]></dc:subject>
		<dc:subject><![CDATA[linked data]]></dc:subject>
		<dc:subject><![CDATA[Open government data]]></dc:subject>
		<dc:subject><![CDATA[perspective]]></dc:subject>
	<description><![CDATA[Simon Rogers, editor of The Guardian&#8216;s Datablog, last week posted a top 10 list data.gov.uk datasets by how they could be relevant to people, highlighting a number of very interesting data sets.Â  He featured national transport statistics, a massive data set cataloging not only every bus, rail, coach stop or pier in the UK but [&#8230;]]]></description>
	<content:encoded><![CDATA[<p><a href="http://www.guardian.co.uk/profile/simonrogers">Simon Rogers</a>, editor of <a title="The Guardian" href="http://www.guardian.co.uk">The Guardian</a>&#8216;s <a href="http://www.guardian.co.uk/news/datablog">Datablog</a>, last week <a href="http://data.gov.uk/blog/my-top-ten-datagovuk-datasets-guest-post-simon-rogers">posted a top 10 list data.gov.uk datasets</a> by how they could be relevant to people, highlighting a number of very interesting data sets.Â  He featured <a href="http://data.gov.uk/dataset/nptdr">national transport statistics</a>, a massive data set cataloging not only every bus, rail, coach stop or pier in the UK but every bus, train, tram, or ferry that docked <a href="http://www.guardian.co.uk/profile/simonrogers">for a weekÂ  in October</a>; as well as statistics on government spending (<a href="http://www.guardian.co.uk/news/datablog/2010/jun/04/coins-database-search">COINS</a>),Â  the UK labour market / employment statistics by year, youth perspectives and attitudes by region, and <a href="http://data.gov.uk/node/11920">statistics on dog messes by UK region</a>.Â  For each he describes a little synopsis of the contents of the data set, highlights its potential uses, and describes problems/limitations of the data.</p>
<p>This post is of tremendous use not because it merely serves to highlight a tiny, delicious morsel from a rather immense soup of more than 4,223 datasets on <a href="http://data.gov.uk">data.gov.uk</a>, but because helps make it relevant to people: he describes (in easily human-understandable terms) what is in each data set, why the data is relevant or interesting, and most importantly, ways that it can be put to use.</p>
<p>The chasm between publishing and use is currently large and daunting.Â  Many of the data sets are in &#8220;raw&#8221; form, Excel spreadsheets created by public servants (using highly specialized government vocabulary) or immense, multi-gigabyte CSV files with little supporting documentation.Â  What we see now is a gold rush (on both sides of the Atlantic &#8211; data.gov and data.gov.uk) of citizen-hackers who are downloading this data, writing scripts to parse through it, and <a href="http://data.gov.uk/apps/list">generating visualisations and apps</a> that make it possible for end-users to actually use it in various ways.</p>
<p>But the <a href="http://www.guardian.co.uk/news/datablog">Guardian Datablog</a> highlights that this might be an ideal role for journalism to come in as well &#8211; while citizen-hackers have been effective at rolling mash-ups that let everyday people get at the data, it still takes a journalist/reporter to get tasty bits out of it &#8212; to transform the raw bits into information &#8211; speculation, perspective, and to contextualize it in world/current events, and to weave it into a story that leads people to question what the data say about the ways they live each day.</p>
<p>These data journalists, of course, do not have to come from Big Media (TV, newspapers) as such &#8211; the ones that do just happen to be best equipped with the right set of skills.Â   In the future, it would be interesting to see whether the many, emerging sense-making and visualisation tools, such as <a href="http://manyeyes.alphaworks.ibm.com/manyeyes/">ManyEyes</a>, <a href="http://tables.googlelabs.com/Home">Google Fusion Tables</a> , <a href="http://www.freebase.com/labs/gridworks">Freebase Gridworks</a>, and our own work, enAKTing&#8217;s GEORDI browser (forthcoming), could make data-journalism more accessible to citizens without a background in statistical data analysis or a journalism degree.Â  If so, these tools could unleashÂ  masses of newly equipped citizen-journalists on the terabytes of open data now publicly available, so that it can be more immediately transformed into information that can start to make an difference in people&#8217;s lives.</p>
]]></content:encoded>
	</item>
<item rdf:about="http://blog.soton.ac.uk/enakting/2010/08/23/a-linked-data-web-of-a-million-easy-pieces/">
	<title>A linked data web of a million easy pieces</title>
	<link>http://blog.soton.ac.uk/enakting/2010/08/23/a-linked-data-web-of-a-million-easy-pieces/</link>
	<dc:date>2010-08-23T02:00:31Z</dc:date>
	<dc:creator><![CDATA[emax]]></dc:creator>
			<dc:subject><![CDATA[linked data]]></dc:subject>
		<dc:subject><![CDATA[perspective]]></dc:subject>
		<dc:subject><![CDATA[software development]]></dc:subject>
		<dc:subject><![CDATA[opinion piece]]></dc:subject>
		<dc:subject><![CDATA[reusable software]]></dc:subject>
	<description><![CDATA[(The following article is an op-ed piece and does not reflect the views of EnAKTing as a whole.) The community of developers and researchers working on the Web of Linked Data are an extraordinary group of talented hackers. Â But anybody who is a member of the community quickly runs into the same problems: the number [&#8230;]]]></description>
	<content:encoded><![CDATA[<p><em>(The following article is an op-ed piece and does not reflect the views of EnAKTing as a whole.)</em></p>
<p>The community of developers and researchers working on the Web of Linked Data are an extraordinary group of talented hackers. Â But anybody who is a member of the community quickly runs into the same problems: the number of tools we have at our disposal areÂ minusculeÂ compared to the massive quantities of &#8220;Web 2.0&#8243; core software tools and development frameworks. Â As a result, we often resort to building things from scratch &#8211; over and over again.</p>
<p>This becomes painfully clear when attempting to teach a new generation of software architects (e.g., university undergraduates) how to build Linked DataÂ systems. Â &#8221;Is &lt; tool &gt; (e.g., <em><a href="http://rdflib.org" target="_blank">rdflib</a></em>) really the <em>only</em> triple store for Python?&#8221; Well, no, I reply, there are a half a dozen others but they are mostly long abandoned, woefully incomplete,Â unstable, buggy, or over-engineered and too complicated to use &#8212; or maybe there are others, but they are insufficiently advertised and unknown. Â How many server frameworks are there for Web 2.0 sites, by comparison? Â Nearly 300, and growing. Â How do you find them? Â Under article entitled <a href="http://en.wikipedia.org/wiki/List_of_web_application_frameworks" target="_blank">Web frameworks</a> on Wikipedia. Â What is their adoption rate? Â Massive. Â How good are they? Â The best power web sites like Twitter, WordPress, and so on: good software.</p>
<p>It is thus no wonder that hundreds of Â Web 2.0 developers are born every second, while the number of new Linked Data systems each year grows by the dozens. Â But not for long: we are on the brink of a phase change, one that involves as much an adoption of Web 2.0 culture by the Linked Data community as the Web 2.0 community has to learn about rich data representations.</p>
<p>The Linked Data community is picking up one important lesson from Web 2.0: simplicity. Â The singular feature of the success of a Web 2.0 framework or toolkit is how easy it is to understand and use. Â  Simplicity begets understandability &#8211; a tool designed to do only one thing is easy to understand. Â The second is robustness &#8212; nobody wants to rely on a system that is buggy or incomplete. Â  These two priorities have pushed the best tools of Web 2.0 (e.g., server frameworks such as <a href="http://djangoproject.org">Django</a> or client-side APIs such as <a href="http://jquery.com" target="_blank">jQuery</a>)Â to become the most widely disseminated and re-used code on the planet. Â Out of these reusable bricks has grown the thousands of random Web-2.0 style social networking and sharing sites we have today.</p>
<p>The tradition the Linked Data community is starting to leave behind, meanwhile is that of building massive, opaque, integrated systems. Â Without wanting to name any here explicitly, one can fairly easily point to massive Linked Data systems that never gained adoption <em>by a single real user </em>because they singularly tried to do too much at once. The lack of ready-made robust tools means that most of these systems started by re-inventing the platform : re-implementing a triple stores, RDF parsers and APIs, DL reasoners, prior to implementing the application or desired user interface. Â In Web 2.0, the equivalent would be roughly designing a web site by re-writing one&#8217;s own HTTP server, web framework or templating language from the ground up &#8211; an obviously time-consuming exercise requiring substantial software development experience.</p>
<p>As a community to move towards the model of building Linked Data applications out of easy pieces as Web 2.0 does, we need to encourage the development of tools and services that are 1) useful 2) simple to use 3) reliable. Â There are so many difficult integration and representation challenges in Linked Data development that the first requirement (finding a need) is trivial. Â The latter two, on the other hand, require considerable thought, design, and (as with any tool for real human users), testing with real developers, iteration and feedback.</p>
<p>One of the core tenets of <a href="http://enakting.org" target="_blank">enAKTing</a> has been to develop simple and essential software and services for the Linked Data web that are easy for all Linked Data developers to use &#8211; from casual to expert developers. Â While we are far from perfecting these tools we&#8217;ve already seen considerable demand for several of these services &#8212; for example, our <a href="http://sameas.org">coreference (&#8220;sameAs&#8221;)</a> service which can be used to find whether two concepts are equivalent; our <a href="http://backlinks.psi.enakting.org/" target="_blank">Backlink service</a> which can be used to find incoming links to concepts on the distributed web of data. Â Our javascript-SPARQL proxy makes it possible for client-side code to directly query SPARQL endpoints &#8211; getting around difficulties such as same-host-restriction policies. Â  We have a host of new other services coming down the pipeline* that, together, we hope will help to usher new innovative Linked Data applications and developers. <span style="font-size: 13.1944px"> </span></p>
<p>But we can&#8217;t do it alone. What tools are you developing? What would you like to see? Let us know.</p>
<p>* Please see <a href="http://enakting.org/services" target="_blank">The enAKting Services</a> for a list of services we are currently developing.</p>
<p><span id="more-13"></span></p>
<p>Max Van Kleek is a Senior Research Fellow and can be contacted at emax &lt;at &gt; ecs dot soton dot ac dot uk</p>
]]></content:encoded>
	</item>
<item rdf:about="http://blog.soton.ac.uk/enakting/2010/08/22/pyclips/">
	<title>Reasoning for the Semantic Web with CLIPS</title>
	<link>http://blog.soton.ac.uk/enakting/2010/08/22/pyclips/</link>
	<dc:date>2010-08-22T15:06:08Z</dc:date>
	<dc:creator><![CDATA[Gianluca]]></dc:creator>
			<dc:subject><![CDATA[software development]]></dc:subject>
	<description><![CDATA[There is a pressing need in the Semantic Web community of exploiting data semantics coming from different hub contexts (e.g. geographic information, personal profiles, place and time) and to put that knowledge to work for ad hoc functionalities and services. The ontological content presents in data silos such as http://data.gov.uk, or http://ordnancesurvey.co.uk shall be firstly [&#8230;]]]></description>
	<content:encoded><![CDATA[<p>There is a pressing need in the Semantic Web community of exploiting data semantics coming from different hub contexts (e.g. geographic information, personal profiles, place and time) and to put that knowledge to work for ad hoc functionalities and services. The ontological content presents in data silos such as http://data.gov.uk, or http://ordnancesurvey.co.uk shall be firstly aligned to a common schema that can be then used to design functionalities and further knowledge bases that contains more personalised concepts such as: good locations for a meal, interesting news for me, or time schedule I need to know.</p>
<p>In classic AI rules based systems are a natural way to express operative knowledge about a domain (even an applicative one) and extend in expressive power classical representational frameworks such as DL ontologies. Reasoning with RDF instances that are distributed can be taunting and the integration of present tools can be tricky. An efficient tool for reasoning with rules that benefits of years of experience and a very efficient RETE implementation is CLIPS. Integrating a CLIPS engine with an existing application is quite easy. Java has its own implementation of CLIPS called Jess, while for the other languages a bridging is provided.</p>
<p><span id="more-8"></span></p>
<p>For python application a module called <a title="pyclips" href="http://pyclips.sourceforge.net/web/" target="_self">pyclips</a> provides access to a CLIPS engine along with a two/way interaction python/CLIPS. It is possible to load and create rules and facts from python, run a CLIPS program and allow CLIPS to call python functions as well. The installation of pyclips is fairly painless (donwload the installing script, configure it and run it; it will automatically download the right version of CLIPS and will install the python module for you).</p>
<p>In order to use the CLIPS engine from python we need to import the <em>clips</em> module and reset it:</p>
<p><code>import clips<br />
clips.Reset()</code><br />
Then we can assert new facts in the environment:</p>
<p><code>clips.Assert('(contains A B)')</code></p>
<p>and new rules:</p>
<p><code>clips.BuildRule('subclass , '(subclass ?a ?b) (subclass ?b ?c)', 'assert((subclass ?a ?c))', 'RDFS subclass transitivity rule')</code></p>
<p>CLIPS has also the capability to define functions and classes. Functions can be called in order to make complex calculations or to create a side effect on the engine with the assertion and retraction of facts.:</p>
<p><code>clips.BuildFunction(	'log', '?message' , '(open "error.log" logdata "a") (printout logdata ?message) (close)' , 'log error message on a log file')</code></p>
<p>And finally we can register python functions to the CLIPS engine and call them from CLIPS rules:</p>
<p><code>def log(message):<br />
log.debug(message)</code></p>
<p><code> </code></p>
<p><code>clips.RegisterPythonFunction(log)<br />
clips.Eval('(python-call log \'error message'\)')</code></p>
<p>Within EnAKTing such a solution has been used for reasoning over topologies, creating a knowledge base of rules that encoded the topology and then letting the engine to simplify it to a canonical form.</p>
<p>The problem to solve was to simplify the alignment between two topologies. The topologies we work on are peculiar ones, they are in fact hierarchical partition of a territory (in this case the UK). This particular condition allows us to assume that a particular region is equal to the union of all of the subregions it contains (i.e. A = B<sub>1</sub> U B<sub>2</sub> U ..U B<sub>n</sub>). The alignment between the NUTS topology and the ONS topology provides three issues:</p>
<ol>
<li>The alignment is not 1:1, that means that sometimes bigger statistical regions are spatially equivalents, sometimes they are not</li>
<li>The alignments available from the European statistical website are provided for the smallest subregions only (i.e. NUTS level 3), whereas nothing is said about spatial equivalence of upper regions</li>
<li>An alignment between the leafs of this topology leaves eventual containments between regions hard to find</li>
</ol>
<p>Using pyclips the two topologies are extracted from the triplestore via SPARQL:</p>
<p><code>SELECT ?super ?sub WHERE {<br />
?super os:contains ?sub<br />
}</code></p>
<p>and then the structure is reorganised in order to collect for each region (<em>top</em>)Â all of its subregions (<em>bottoms</em>).Â For example, the NUTS region UKH32 (<em><a href="http://upload.wikimedia.org/wikipedia/commons/1/16/EnglandThurrock.png" target="_blank">Thurrock</a></em>) has twenty subregions within the ONS hierarchy:</p>
<ul>
<li>00KGMX	<em>Aveley and Uplands</em></li>
<li>00KGMY	<em>Belhus</em></li>
<li>00KGMZ	<em>Chadwell St Mary</em></li>
<li>00KGNA<em> Chafford and North Stifford</em></li>
<li>00KGNB	<em>Corringham and Fobbing</em></li>
<li>00KGNC	<em>East Tilbury</em></li>
<li>00KGND	<em>Grays Riverside</em></li>
<li>00KGNE	<em>Grays Thurrock</em></li>
<li>00KGNF	<em>Little Thurrock Blackshots</em></li>
<li>00KGNG	<em>Little Thurrock Rectory</em></li>
<li>00KGNH	<em>Ockendon</em></li>
<li>00KGNJ	<em>Orsett</em></li>
<li>00KGNK	<em>South Chafford</em></li>
<li>00KGNL	<em>Stanford East and Corringham Town</em></li>
<li>00KGNM	<em>Stanford-le-Hope West</em></li>
<li>00KGNN	<em>Stifford Clays</em></li>
<li>00KGNP	<em>The Homesteads</em></li>
<li>00KGNQ	<em>Tilbury Riverside and Thurrock Park</em></li>
<li>00KGNR	<em>Tilbury St Chads</em></li>
<li>00KGNS	<em>West Thurrock and South Stifford</em></li>
</ul>
<p>and a rule in CLIPS is created in order to simplify eventual topology expressions including the above listed ONS regions. The code in pyclips is the following:<br />
<code>for top , bottoms in structure.iteritems():<br />
for b in bottoms:<br />
f = clips.Assert('(contains %s %s)'%(top,b))<br />
r = clips.BuildRule('reduce_%s'%ri , ''.join(['(contains  ?t&amp;amp;~%s %s)'%(top,b) for b in bottoms]),<br />
'(assert (contains ?t %s)) (python-call acontains ?t %s)'%(top,top),<br />
'reducing rule')</code></p>
<p>and the rule generated for the above case is the following:</p>
<p><code>(defrule MAIN::reduce_769 "reducing rule"<br />
(contains ?t&amp;amp;~UKH32 00KGNL)<br />
(contains ?t&amp;amp;~UKH32 00KGNR)<br />
(contains ?t&amp;amp;~UKH32 00KGNM)<br />
(contains ?t&amp;amp;~UKH32 00KGNK)<br />
(contains ?t&amp;amp;~UKH32 00KGNF)<br />
(contains ?t&amp;amp;~UKH32 00KGNG)<br />
(contains ?t&amp;amp;~UKH32 00KGNE)<br />
(contains ?t&amp;amp;~UKH32 00KGNA)<br />
(contains ?t&amp;amp;~UKH32 00KGMZ)<br />
(contains ?t&amp;amp;~UKH32 00KGNN)<br />
(contains ?t&amp;amp;~UKH32 00KGNP)<br />
(contains ?t&amp;amp;~UKH32 00KGNJ)<br />
(contains ?t&amp;amp;~UKH32 00KGNH)<br />
(contains ?t&amp;amp;~UKH32 00KGNC)<br />
(contains ?t&amp;amp;~UKH32 00KGND)<br />
(contains ?t&amp;amp;~UKH32 00KGNB)<br />
(contains ?t&amp;amp;~UKH32 00KGMY)<br />
(contains ?t&amp;amp;~UKH32 00KGMX)<br />
(contains ?t&amp;amp;~UKH32 00KGNQ)<br />
(contains ?t&amp;amp;~UKH32 00KGNS)<br />
=&amp;gt;<br />
(assert (contains ?t UKH32))<br />
(python-call acontains ?t UKH32))</code><br />
This rule will match a region that contains all the subregions that UKH32 contains (but not UK32 itself). If so it will assert a new relation in the knowledge base saying that such region contains UKH32 as well.<br />
A further rule will cover the scenario where two regions contains the same subregions and only those. In fact, in this case Â the engine will infer that A contains B AND B contains A:<br />
<code>clips.RegisterPythonFunction(aequals)<br />
clips.RegisterPythonFunction(acontains)</code></p>
<p><code> </code></p>
<p><code>clips.BuildRule('symmetry',<br />
'?f1 &amp;lt;- (contains ?a ?b) ?f2&amp;lt;-(contains ?b ?a)' ,<br />
'(retract ?f1) (retract ?f2) (assert (spatially_equal ?a ?b) (spatially_equal ?b ?a)) (python-call aequals ?a ?b)',<br />
'symetry reducing rule')</code></p>
<p>Once the topology has been converted to a CLIPS program the only thing to do is to run the program and the CLIPS engine will call the callback functions <strong>aequals</strong> and <strong>acontains</strong> every time it simplify an expression in accordance with the rules.</p>
<p><code>clips.Run()</code></p>
<p>Gianluca Correndo</p>
<p>Research fellow at ECS, University of Southampton</p>
<p>The author can be contacted at mailto:gc3 &lt; at &gt; ecs.soton.ac.uk</p>
]]></content:encoded>
	</item>
<item rdf:about="http://blog.soton.ac.uk/enakting/2009/11/04/welcome_to_enakting/">
	<title>Welcome to EnAKTing</title>
	<link>http://blog.soton.ac.uk/enakting/2009/11/04/welcome_to_enakting/</link>
	<dc:date>2009-11-04T10:40:04Z</dc:date>
	<dc:creator><![CDATA[enakting]]></dc:creator>
			<dc:subject><![CDATA[Uncategorized]]></dc:subject>
	<description><![CDATA[Welcome to the EPSRC-funded &#8220;EnAKTing project&#8221;, looking at challenges of the unbounded data Web. The development of new Semantic Web technologies points to a new generation of Web capability that can explore and query, assemble, and integrate content in a context-aware, focused fashion. The basic idea is that we move from a document centric view [&#8230;]]]></description>
	<content:encoded><![CDATA[<p>Welcome to the EPSRC-funded &#8220;<a href="http://gow.epsrc.ac.uk/ViewGrant.aspx?GrantRef=EP/G008493/1">EnAKTing project</a>&#8221;, looking at challenges of the unbounded data Web.</p>
<p>The development of new Semantic Web technologies points to a new generation of Web capability that can explore and query, assemble, and integrate content in a context-aware, focused fashion. The basic idea is that we move from a document centric view of the Web to one in which data and information are the principle objects of interest.<br />
With the emergence of a Web of data it is essential to address three key research problems, viz (1) how to build ontologies quickly that are capable of exploiting the potential of large-scale user participation, (2) how we query an unbounded web of linked data, (3) how to visualise, explore, browse and navigate this mass of data.</p>
<p>In the EnAKTing project, we are undertaking fundamental research in the above three areas.</p>
]]></content:encoded>
	</item>
</rdf:RDF>
